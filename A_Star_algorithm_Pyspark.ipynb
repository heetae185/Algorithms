{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk+Vm8ZUi7B6DZ2N4VeZF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heetae185/Algorithms/blob/main/A_Star_algorithm_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz5yyDY192S1"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pyspark\n",
        "!pip install --quiet graphframes\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o \"/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes0.8.2-spark3.2-s_2.12.jar\" http://dl.bintray.com/sparkpackages/maven/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar"
      ],
      "metadata": {
        "id": "iyrUGkJkGpDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from graphframes import *"
      ],
      "metadata": {
        "id": "cnOVQ7sv-miZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sc = pyspark.SparkContext()\n",
        "spark = SparkSession.builder.master('local[*]').config('spark.jars.packages', 'graphframes:graphframes:0.8.2-spark3.2-s_2.12').getOrCreate()"
      ],
      "metadata": {
        "id": "hFSFgMDrFwo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1wMZdMz3m4OIHNH-b3zgEaZrNV2GdNpre # USA-road-d.NY.gr\n",
        "!gdown 1P3BiMuJvMOqdhVsRI9E1EfKsd7cLrGBs # USA-road-d.NY.co"
      ],
      "metadata": {
        "id": "3CzHiROL-qNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(filename, firstletter):\n",
        "  text_col = ''\n",
        "  with open(filename, 'r') as f:   \n",
        "    while True:\n",
        "      line = f.readline()\n",
        "      if not line : break\n",
        "      if line[0] == firstletter:\n",
        "        text = line.split(' ')\n",
        "        text_line = text[1] + ',' + text[2] + ',' + text[3]\n",
        "        text_col += text_line\n",
        "  f.close()\n",
        "  filename_string = filename.split('.')\n",
        "  filename = filename_string[0] + '_processed.' + filename_string[1] + '.' + filename_string[2]\n",
        "  with open(filename, 'w') as f:\n",
        "    f.write(text_col)\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "16c8Q2cUFEDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocess('USA-road-d.NY.co', 'v')\n",
        "text_preprocess('USA-road-d.NY.gr', 'a')"
      ],
      "metadata": {
        "id": "ajO4fzJL6xqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "co = pd.read_csv('USA-road-d_processed.NY.co')\n",
        "co.head(5)"
      ],
      "metadata": {
        "id": "W5Mah6sn_VNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프 프레임 선언\n",
        "def create_transport_graph():\n",
        "  node_fields = [\n",
        "      StructField(\"id\", IntegerType(), True),\n",
        "      StructField(\"longitude\", LongType(), True),\n",
        "      StructField(\"latitude\", LongType(), True)\n",
        "  ]\n",
        "  nodes = spark.read.csv(\"USA-road-d_processed.NY.co\", header=False, sep=',', schema=StructType(node_fields))\n",
        "\n",
        "  rel_fields = [\n",
        "      StructField(\"src\", IntegerType(), True),\n",
        "      StructField(\"dst\", IntegerType(), True),\n",
        "      StructField(\"distance\", IntegerType(), True)\n",
        "  ]\n",
        "  rels = spark.read.csv(\"USA-road-d_processed.NY.gr\", header=False, sep=',', schema=StructType(rel_fields))\n",
        "\n",
        "  return GraphFrame(nodes, rels)"
      ],
      "metadata": {
        "id": "NXuZ4O_1x8bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = create_transport_graph()"
      ],
      "metadata": {
        "id": "eyxzrgVN0_Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g.vertices.show()"
      ],
      "metadata": {
        "id": "h-stx76ENwhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g.edges.show()"
      ],
      "metadata": {
        "id": "EPzEaCfSORbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o_table = g.vertices.alias('o_table')\n",
        "i_table = g.vertices.alias('i_table')\n",
        "new_vertices = g.vertices.alias('vertices')"
      ],
      "metadata": {
        "id": "xZWsDtXVT5Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vertices.show()"
      ],
      "metadata": {
        "id": "v0vKKCwXU5_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# 두 노드 간 거리 구하는 함수\n",
        "def get_distance(src_id, dst_id):\n",
        "  src_lon = g.vertices.filter(g.vertices.id == src_id).first().longitude\n",
        "  src_lat = g.vertices.filter(g.vertices.id == src_id).first().latitude\n",
        "  dst_lon = g.vertices.filter(g.vertices.id == dst_id).first().longitude\n",
        "  dst_lat = g.vertices.filter(g.vertices.id == dst_id).first().latitude\n",
        "  return math.sqrt((src_lon - dst_lon)**2 + (src_lat - dst_lat)**2)"
      ],
      "metadata": {
        "id": "KhJ7VsP8V4Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자신의 node와 연결되어 있는 node 구하기\n",
        "def directed_list(node_id, dst_id):\n",
        "  edge = g.edges.filter(g.edges.src == node_id)\n",
        "  direct_list = new_vertices.join(edge, new_vertices.id == edge.src, 'inner').select('dst', 'longitude', 'latitude', 'distance', 'id').withColumnRenamed('id', 'parentNode').withColumnRenamed('dst', 'id').withColumnRenamed('distance', 'gscore')\n",
        "  expected_remain_distance = [(direct.id, get_distance(direct.id, dst_id)) for direct in direct_list.collect()]\n",
        "  schema = StructType([\n",
        "      StructField('id', IntegerType(), True),\n",
        "      StructField('hscore', FloatType(), True),\n",
        "  ])\n",
        "  rdd = spark.sparkContext.parallelize(expected_remain_distance)\n",
        "  hscore = spark.createDataFrame(rdd,schema)\n",
        "  direct_list = direct_list.join(hscore, on='id', how='inner').select('id', 'gscore', 'hscore', 'parentNode')\n",
        "  direct_list = direct_list.withColumn('fscore', direct_list.hscore + direct_list.gscore).select('id', 'fscore', 'gscore', 'hscore', 'parentNode')\n",
        "  return direct_list"
      ],
      "metadata": {
        "id": "X7k1cKgZqYXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path_finder(src_id, dst_id):\n",
        "\n",
        "  schema = StructType([\n",
        "      StructField('id', IntegerType(), True),\n",
        "      StructField('fscore', FloatType(), True),\n",
        "      StructField('gscore', FloatType(), True),\n",
        "      StructField('hscore', FloatType(), True),\n",
        "      StructField('parentNode', IntegerType(), True)\n",
        "  ])\n",
        "\n",
        "  close_list = g.vertices.filter(g.vertices.id == src_id)\\\n",
        "  .withColumn('fscore', F.lit(float('inf'))).withColumn('gscore', F.lit(float('inf'))).withColumn('hscore', F.lit(float('inf'))).withColumn('parentNode', F.lit(0))\\\n",
        "  .select('id', 'fscore', 'gscore', 'hscore', 'parentNode')\n",
        "\n",
        "  open_list = directed_list(src_id, dst_id).sort('fscore')\n",
        "  opened_id = open_list.first().id\n",
        "\n",
        "  def update_open_list(close_list, closed_id, open_list):\n",
        "    candidate_list = directed_list(closed_id, dst_id)\n",
        "    close_list_id = [c.id for c in close_list.collect()]\n",
        "    union_list = open_list.union(candidate_list)\n",
        "    union_list.sort('fscore').coalesce(1).dropDuplicates(['id'])\n",
        "    open_list = union_list.filter(~union_list.id.isin(close_list_id)).sort('fscore')\n",
        "    opened_id = open_list.first().id\n",
        "    return open_list, opened_id, close_list\n",
        "\n",
        "  def update_close_list(open_list, opened_id, close_list):\n",
        "    new_close = [(open_list.first().id, open_list.first().fscore, float(open_list.first().gscore), open_list.first().hscore, open_list.first().parentNode)]\n",
        "    open_top = spark.createDataFrame(new_close, schema)\n",
        "    close_list = close_list.union(open_top)\n",
        "    closed_id = new_close[0][0]\n",
        "    open_list = open_list.filter(open_list.id != closed_id)\n",
        "    return close_list, closed_id, open_list\n",
        "\n",
        "  while True:\n",
        "    close_list, closed_id, open_list = update_close_list(open_list, opened_id, close_list)\n",
        "    open_list, opened_id, close_list = update_open_list(close_list, closed_id, open_list)\n",
        "    print('-------------------')\n",
        "    print(close_list.collect())\n",
        "    print('-------------------')\n",
        "    print(open_list.collect())\n",
        "    if dst_id in [close.id for close in close_list.collect()]:\n",
        "      route = [dst_id]\n",
        "      key_id = dst_id\n",
        "      while key_id != src_id:\n",
        "        temp_id = close_list.filter(close_list.id == key_id).first().parentNode\n",
        "        key_id = temp_id\n",
        "        route.append(key_id)\n",
        "      route = list(reversed(route))\n",
        "      break\n",
        "\n",
        "  return route"
      ],
      "metadata": {
        "id": "AzB4oHk0nd_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path_find_bot():\n",
        "  print('---------------------------------------------------------')\n",
        "  start = input('출발 노드 : ')\n",
        "  end = input('도착 노드 : ')\n",
        "  print('경로 : ', path_finder(start, end))\n",
        "  print('---------------------------------------------------------')"
      ],
      "metadata": {
        "id": "Jd-FAH6-0tpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_find_bot()"
      ],
      "metadata": {
        "id": "JEl5G6W4ZSqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}